{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press keys or click the mouse (Press 'Esc' to exit)\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "left\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "left\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "left\n",
      "q\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "q\n",
      "q\n",
      "left\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "d\n",
      "left\n",
      "d\n",
      "d\n",
      "left\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "left\n",
      "q\n",
      "left\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "d\n",
      "d\n",
      "left\n",
      "q\n",
      "q\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mss.tools\n",
    "import time\n",
    "import uuid\n",
    "import keyboard\n",
    "import math\n",
    "import pyautogui\n",
    "import time\n",
    "import mss.tools\n",
    "import uuid\n",
    "import keyboard\n",
    "from pynput import mouse\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "def screenshot(folder):\n",
    "    with mss.mss() as sct:\n",
    "        monitor_info = sct.monitors[0]\n",
    "        top_margin = (monitor_info[\"height\"] // 4) + 50\n",
    "        monitor = {\"top\": monitor_info[\"top\"] + top_margin, \"left\": monitor_info[\"left\"] + 55, \"width\": monitor_info[\"width\"] - 120, \"height\": (monitor_info[\"height\"] * 2 // 3)-105}\n",
    "        output = f\"deep_learning_data/{folder}/{str(uuid.uuid4())}.png\".format(**monitor)\n",
    "        sct_img = sct.grab(monitor)\n",
    "        mss.tools.to_png(sct_img.rgb, sct_img.size, output=output)\n",
    "    return output\n",
    "        \n",
    "\n",
    "def on_keyboard_event(event):\n",
    "    print(f\"{event.name}\")\n",
    "    return screenshot(event.name)\n",
    "    \n",
    "def on_mouse_event(x, y, button, pressed):    \n",
    "    print(f\"{button.name}\")\n",
    "    return screenshot('forward')\n",
    "\n",
    "keyboard.hook(on_keyboard_event)\n",
    "\n",
    "with mouse.Listener(on_click=on_mouse_event) as listener:\n",
    "    print(\"Press keys or click the mouse (Press 'Esc' to exit)\")\n",
    "    keyboard.wait('space')  # Wait for the 'esc' key to exit the program\n",
    "    listener.stop()  # Stop the mouse listener\n",
    "    keyboard.unhook_all()  # Unhook the keyboard eventsdq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d - Train: 330 | Test: 83\n",
      "forward - Train: 89 | Test: 23\n",
      "q - Train: 191 | Test: 48\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = 'deep_learning_data'\n",
    "\n",
    "# Get the list of class folders (forward, d, q)\n",
    "class_folders = [folder for folder in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, folder))]\n",
    "\n",
    "# Initialize dictionaries to store the paths of images for each class\n",
    "class_images = {class_folder: [] for class_folder in class_folders}\n",
    "\n",
    "# Populate the dictionaries with image paths\n",
    "for class_folder in class_folders:\n",
    "    class_folder_path = os.path.join(dataset_path, class_folder)\n",
    "    class_images[class_folder] = [os.path.join(class_folder_path, image) for image in os.listdir(class_folder_path)]\n",
    "\n",
    "# Split the data into train and test sets for each class\n",
    "train_data, test_data = {}, {}\n",
    "for class_folder in class_folders:\n",
    "    train_data[class_folder], test_data[class_folder] = train_test_split(class_images[class_folder], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the number of samples in each split\n",
    "for class_folder in class_folders:\n",
    "    print(f'{class_folder} - Train: {len(train_data[class_folder])} | Test: {len(test_data[class_folder])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d - Train: 330 | Test: 83\n",
      "forward - Train: 89 | Test: 23\n",
      "q - Train: 191 | Test: 48\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = 'deep_learning_data'\n",
    "\n",
    "# Get the list of class folders (forward, d, q)\n",
    "class_folders = [folder for folder in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, folder))]\n",
    "\n",
    "# Create train and test folders for each class\n",
    "for class_folder in class_folders:\n",
    "    train_folder = os.path.join(dataset_path, f'{class_folder}_train')\n",
    "    test_folder = os.path.join(dataset_path, f'{class_folder}_test')\n",
    "    \n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Initialize dictionaries to store the paths of images for each class\n",
    "class_images = {class_folder: [] for class_folder in class_folders}\n",
    "\n",
    "# Populate the dictionaries with image paths\n",
    "for class_folder in class_folders:\n",
    "    class_folder_path = os.path.join(dataset_path, class_folder)\n",
    "    class_images[class_folder] = [os.path.join(class_folder_path, image) for image in os.listdir(class_folder_path)]\n",
    "\n",
    "# Split the data into train and test sets for each class\n",
    "train_data, test_data = {}, {}\n",
    "for class_folder in class_folders:\n",
    "    train_data[class_folder], test_data[class_folder] = train_test_split(class_images[class_folder], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Copy images to train folder\n",
    "    for img_path in train_data[class_folder]:\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        dest_path = os.path.join(dataset_path, f'{class_folder}_train', img_filename)\n",
    "        shutil.copy(img_path, dest_path)\n",
    "\n",
    "    # Copy images to test folder\n",
    "    for img_path in test_data[class_folder]:\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        dest_path = os.path.join(dataset_path, f'{class_folder}_test', img_filename)\n",
    "        shutil.copy(img_path, dest_path)\n",
    "\n",
    "# Print the number of samples in each split\n",
    "for class_folder in class_folders:\n",
    "    print(f'{class_folder} - Train: {len(train_data[class_folder])} | Test: {len(test_data[class_folder])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "input_folder = './deep_learning_data'\n",
    "output_folder = './split_deep_learning_data'\n",
    " \n",
    "#split the images into 80% training and 20% testing sets\n",
    "splitfolders.ratio(input_folder, output_folder, seed=42, ratio=(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dinab\\Self Driving Mario Cart\\Self-Driving-Mario-Kart\\mario_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 610 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Load and augment the training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'split_deep_learning_data/train',\n",
    "    target_size=(224, 224),  # Adjust the target size based on your model\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dinab\\Self Driving Mario Cart\\Self-Driving-Mario-Kart\\mario_env\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\dinab\\Self Driving Mario Cart\\Self-Driving-Mario-Kart\\mario_env\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 18s 0us/step\n",
      "WARNING:tensorflow:From c:\\Users\\dinab\\Self Driving Mario Cart\\Self-Driving-Mario-Kart\\mario_env\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Build your model on top of the ResNet base\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes: forward, d, q\n",
    "\n",
    "# Freeze the layers of the ResNet base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\dinab\\Self Driving Mario Cart\\Self-Driving-Mario-Kart\\mario_env\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\dinab\\Self Driving Mario Cart\\Self-Driving-Mario-Kart\\mario_env\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "20/20 [==============================] - 62s 3s/step - loss: 1.1916 - accuracy: 0.3934\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 47s 2s/step - loss: 1.0118 - accuracy: 0.5410\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 52s 3s/step - loss: 0.9927 - accuracy: 0.5443\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 51s 3s/step - loss: 0.9853 - accuracy: 0.5410\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 47s 2s/step - loss: 0.9946 - accuracy: 0.5410\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.9784 - accuracy: 0.5410\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.9974 - accuracy: 0.4820\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 45s 2s/step - loss: 0.9975 - accuracy: 0.5443\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.9759 - accuracy: 0.5410\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 45s 2s/step - loss: 1.0062 - accuracy: 0.5115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d591671690>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  # Adjust the number of epochs based on your dataset and training needs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 154 images belonging to 3 classes.\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.9893 - accuracy: 0.5390\n",
      "Validation Accuracy: 53.90%\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess validation data\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'split_deep_learning_data/val',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dinab\\Self Driving Mario Cart\\Self-Driving-Mario-Kart\\mario_env\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, save_model\n",
    "\n",
    "# Assuming your model is already trained and stored in the 'model' variable\n",
    "# Train your model ...\n",
    "\n",
    "# Save the model to an HDF5 file\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# To load the model back later, you can use the following:\n",
    "# loaded_model = load_model('your_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 0, 'forward': 1, 'q': 2}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "0\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 323ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 371ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import keyboard\n",
    "\n",
    "# Load your pre-trained model\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(img):\n",
    "    # Resize the image to match the input size of your model\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    # Normalize pixel values to be in the range [0, 1]\n",
    "    img = img / 255.0\n",
    "    # Expand dimensions to match the input shape expected by the model\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Start the game (replace 'start_game_command' with the actual command to start your game)\n",
    "# This command could be launching the emulator or opening the game window\n",
    "start_game_command = 'start_game_command'\n",
    "# Uncomment the line below if you need to launch the game\n",
    "os.system(start_game_command)\n",
    "\n",
    "isTrue = True\n",
    "# Main loop\n",
    "while isTrue:\n",
    "    # Capture the game screen\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    frame = np.array(screenshot)\n",
    "    \n",
    "    # Convert RGB to BGR (OpenCV uses BGR)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Preprocess the image\n",
    "    processed_image = preprocess_image(frame)\n",
    "    \n",
    "    # Make a prediction using your model\n",
    "    prediction = model.predict(processed_image)\n",
    "    \n",
    "    # Determine the action based on the prediction\n",
    "    action_index = np.argmax(prediction)\n",
    "\n",
    "    print(action_index)\n",
    "    # Perform the corresponding action\n",
    "    if action_index == 0:  # Replace 0 with the index for 'forward' class\n",
    "        # Press the mouse (assuming it's equivalent to 'forward' action in your game)\n",
    "        keyboard.press('q')\n",
    "        pyautogui.mouseDown()\n",
    "        time.sleep(0.15)\n",
    "        pyautogui.mouseUp()\n",
    "        keyboard.release('q')\n",
    "    elif action_index == 2:  # Replace 2 with the index for 'q' class\n",
    "        # Press 'q' key\n",
    "        keyboard.press('d')\n",
    "        pyautogui.mouseDown()\n",
    "        time.sleep(0.15)\n",
    "        pyautogui.mouseUp()\n",
    "        keyboard.redlease('d')\n",
    "    \n",
    "    if keyboard.is_pressed(\"space\"):\n",
    "        isTrue = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mario_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
