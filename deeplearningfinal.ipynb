{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgMqp926T0_z"
      },
      "source": [
        "# Deep Learning Self Driving Car\n",
        "#### Team members: Ingrid Hansen, Iris Loret, Dina Boshnaq\n",
        "#### Group 2\n",
        "\n",
        "## Introduction\n",
        "For the deep learning model, we chose the same track as for OpenCV. We are first going to collect the data and label it according to which key was pressed. After that we will create the model and also train it. After that we will evaluate it by looking at the confusion matrix. And lastly, it's time to capture the screen and give that to our model to make predictions to what direction he should drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "d8CwoWsEAyY0",
        "outputId": "05c17c56-5b61-48d9-cfd4-7774c1b0803e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ingri\\Documents\\3ITF\\AICourse\\Project\\Self-Driving-Mario-Kart\\car_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Driving imports\n",
        "import cv2\n",
        "import pyautogui\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "import keyboard\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mss.tools\n",
        "import time\n",
        "import uuid\n",
        "import keyboard\n",
        "import pyautogui\n",
        "import time\n",
        "import mss.tools\n",
        "import uuid\n",
        "import keyboard\n",
        "from pynput import mouse\n",
        "#Model Imports\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Reshape\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import resnet_v2\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eow44OpW-15"
      },
      "source": [
        "## Collecting data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvg-Hq1qXCZX"
      },
      "source": [
        "To collect data for our model we utilized the code below. The code takes a screenshot within the boundaries specified in the code. For us this is about one third of the screen position in the centre of the y axis of the screen.\n",
        "\n",
        "The screenshot function takes a folder as a parameter. This folder parameter is collected through key inputs while playing the game. Through this setup the images are saved in folders corresponding to the keys that are pressed while the game is being played. Through this our data is sorted with the correct label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk7Ngy5SXCvl"
      },
      "outputs": [],
      "source": [
        "time.sleep(5)\n",
        "\n",
        "def screenshot(folder):\n",
        "    with mss.mss() as sct:\n",
        "      # Select monitor index\n",
        "        monitor_info = sct.monitors[0]\n",
        "        # Create a margin for the top of the screen so the area of interest is positioned correctly on the y-axis.\n",
        "        top_margin = (monitor_info[\"height\"] // 4) + 50\n",
        "        # Dictionary where the portion of the monitor is determined. We add margins to correctly capture the area of interest.\n",
        "        monitor = {\"top\": monitor_info[\"top\"] + top_margin, \"left\": monitor_info[\"left\"] + 55, \"width\": monitor_info[\"width\"] - 120, \"height\": (monitor_info[\"height\"] * 2 // 3)-450}\n",
        "        # The output where the images are saved. The subfolder corresponds to the input, then a randomly generated file name is created.\n",
        "        output = f\"citydata/{folder}/{str(uuid.uuid4())}.png\".format(**monitor)\n",
        "        sct_img = sct.grab(monitor)\n",
        "        mss.tools.to_png(sct_img.rgb, sct_img.size, output=output)\n",
        "    return output\n",
        "\n",
        "#Track keyboard input\n",
        "def on_keyboard_event(event):\n",
        "    print(f\"{event.name}\")\n",
        "    return screenshot(event.name)\n",
        "#Track mouse input\n",
        "def on_mouse_event(x, y, button, pressed):\n",
        "    print(f\"{button.name}\")\n",
        "    return screenshot('forward')\n",
        "\n",
        "keyboard.hook(on_keyboard_event)\n",
        "with mouse.Listener(on_click=on_mouse_event) as listener:\n",
        "    print(\"Press keys or click the mouse (Press 'Esc' to exit)\")\n",
        "    keyboard.wait('space')  # Wait for the 'space' key to exit the program\n",
        "    listener.stop()  # Stop the mouse listener\n",
        "    keyboard.unhook_all()  # Unhook the keyboard events\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display an image of every class:\n",
        "\n",
        "Right (d): \n",
        "\n",
        "<img src=\"./tracksplit/train/d/0197a1b5-8c2b-48da-914d-897113e39e6c.png\" width=\"700\">\n",
        "\n",
        "Left (q):\n",
        "\n",
        "<img src=\"./tracksplit/train/q/043351db-3297-475b-b274-de9ae166c1cc.png\" width=\"700\">\n",
        "\n",
        "Forward (f):\n",
        "\n",
        "<img src=\"./tracksplit/train/f/002d69d1-8a13-4a47-86b4-ac3b3afea1ff.png\" width=\"700\">\n",
        "\n",
        "Correct right (h):\n",
        "\n",
        "<img src=\"./tracksplit/train/h/001b5118-5f43-4939-9287-a5fc04350919.png\" width=\"700\">\n",
        "\n",
        "Correct left (j):\n",
        "\n",
        "<img src=\"./tracksplit/train/j/2750a150-0acf-47d2-8f6d-4d164a53c7cf.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKxzc60OXNUa"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWf32VIvmhYJ"
      },
      "source": [
        "When deciding on a pretrained model to build our own model on we first tried out a ResNet50 model. However, with this model we could only reach about 40% accuracy with the model. After trying out various methods to increase the accuracy and conducting research we tried out some other pretrained models. Other versions of ResNet were tried and we eventually reached ResNet50V2. With this pretrained model we could reach about 93% accuracy with our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Yu6XSqnf-J"
      },
      "source": [
        "The data we collected is split into training and testing data. As previously stated the images are stored in folders corresponding to their labels. This dataset contains five labels:\n",
        "\n",
        "'d': Right\n",
        "\n",
        "'q': Left\n",
        "\n",
        "'f': Forward\n",
        "\n",
        "'h': Correct right\n",
        "\n",
        "'j': Correct left\n",
        "\n",
        "Right means that there is a turn to the right and Mario should go to the right. Left means the opposite and forward means just to go forward. Correct right is for when Mario is close to the right side of the track and should go to the left. Correct left then means that Mario is close to the left side of the track and needs to go to the right.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f2m7XEyQAyY3"
      },
      "outputs": [],
      "source": [
        "# Split the images into training and testing sets\n",
        "import splitfolders\n",
        "input_folder = 'trackimages'\n",
        "output_folder = 'tracksplit'\n",
        "\n",
        "# 80% training and 20% testing \n",
        "splitfolders.ratio(input_folder, output_folder, seed=40, ratio=(0.8, 0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9dcquhMoejz"
      },
      "source": [
        "The ResNet50V2 model will be used with the imagenet weights. The top layer, which is the final fully connected layer, is excluded because our own fully connected layer is added. Our custom layer is tailored to the needs of our specific task. The input shape is set to an image size of 224x224 with three channels. This is the required image dimensions to use ResNet with imagenet.\n",
        "\n",
        "Before setting up our own layers we set the pre-existing weights in the model to not be trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "uPDTgxn0AyY3"
      },
      "outputs": [],
      "source": [
        "model = ResNet50V2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "_d0cSfuTAyY3"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6-2BgN_p5tq"
      },
      "source": [
        "The output tensor of the model is retrieved and used as the input for the layers we add onto the model. We apply batch normalization on the inputs. Following this, a convolutional layer is added with 64 filters of the size 3x3. Max Pooling is performed followed by flattening the input. Two Dense layers are then added with 128 and 64 neurons respectively. The layers use ReLu as an activation function. The fully connected layer we add is specified to have five neurons, each corresponding to each potential output. The optimizer for the model is adam with a learning rate of 0.01. The model is compiled with the categorical crossentropy as a loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "UB4dbi0mAyY3"
      },
      "outputs": [],
      "source": [
        "x = model.output\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x) \n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "classes = Dense(5, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=model.input, outputs=classes)\n",
        "optimizer = optimizers.Adam(learning_rate=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "7j-x0ATBAyY3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICBNyBzPuLir"
      },
      "source": [
        "When generating image data we perform some data augmentation. The pixel values are normalized to a 0-1 range. The images can be shifted on the width and height by 10% and can be zoomed in or out by 10%. The channel shift means that the color channels in the image could be increased or decreased by up to 20%. The brigthness can also be adjusted up to 20% up or down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "xKp1uCHYAyY4"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                width_shift_range=0.1,\n",
        "                                height_shift_range=0.1,\n",
        "                                zoom_range=0.1,\n",
        "                                channel_shift_range=0.2,\n",
        "                                brightness_range=(0.8, 1.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HksbEChcAyY4",
        "outputId": "ce6ab3f6-4bdf-46b7-9413-3156ec0e254c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 874 images belonging to 5 classes.\n",
            "Found 220 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "# Specify the train and validation dataset\n",
        "train_data = datagen.flow_from_directory(\"tracksplit/train\", target_size=(224, 224), batch_size=120, class_mode=\"categorical\")\n",
        "val_data = datagen.flow_from_directory(\"tracksplit/val\", target_size=(224, 224), batch_size=120, class_mode=\"categorical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0aIoezWAyY4",
        "outputId": "68c39b61-28f0-43db-8549-121ac728542f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "8/8 [==============================] - 169s 20s/step - loss: 8.3119 - accuracy: 0.5206 - val_loss: 3.3112 - val_accuracy: 0.7227\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 177s 21s/step - loss: 1.9422 - accuracy: 0.7872 - val_loss: 2.3875 - val_accuracy: 0.7409\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 169s 21s/step - loss: 0.9444 - accuracy: 0.8272 - val_loss: 1.1077 - val_accuracy: 0.8636\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 152s 19s/step - loss: 0.4816 - accuracy: 0.8913 - val_loss: 0.4668 - val_accuracy: 0.8727\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 172s 21s/step - loss: 0.2866 - accuracy: 0.9211 - val_loss: 0.3712 - val_accuracy: 0.8545\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7979dd0079a0>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_data, epochs=5, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "g03GL5NWHc3N"
      },
      "outputs": [],
      "source": [
        "# Take an image from the validation set to test\n",
        "img = image.load_img(\"tracksplit/val/j/0fa740d2-3491-4e1b-8159-07faacb6f634.png\", target_size=(224, 224))\n",
        "                     \n",
        "# Convert the image to a numpy array\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Expand the dimensions to match the shape that the model expects\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# Preprocess the input\n",
        "x = resnet_v2.preprocess_input(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJlgLDJBIAJD",
        "outputId": "cf7063f5-2667-4bcd-a535-6becf3bb2187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 124ms/step\n"
          ]
        }
      ],
      "source": [
        "# Predict a direction based on the image\n",
        "direction = model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfXY6dcRHqO1",
        "outputId": "fc32a068-8c68-48d6-8dfb-12faf7802644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "0.99866915\n"
          ]
        }
      ],
      "source": [
        "# The action index: 0=d, 1=f, 2=h, 3=j, 4=q\n",
        "action_index = np.argmax(direction)\n",
        "# How confident is the model in the predicted direction\n",
        "confidence = direction[0][action_index]\n",
        "print(action_index)\n",
        "print(confidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7jfi71bHrOh",
        "outputId": "0490b921-8e40-4c59-8c70-11edaa14bdf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 888ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 364ms/step\n",
            "1/1 [==============================] - 0s 275ms/step\n",
            "1/1 [==============================] - 0s 296ms/step\n",
            "1/1 [==============================] - 0s 343ms/step\n",
            "1/1 [==============================] - 0s 362ms/step\n",
            "1/1 [==============================] - 0s 343ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "1/1 [==============================] - 0s 266ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 202ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "[[48  8  0  0  0]\n",
            " [ 2 55  0  0  3]\n",
            " [ 0  0 17  4  0]\n",
            " [ 6  2  0 26  0]\n",
            " [ 0  3  0  0 46]]\n"
          ]
        }
      ],
      "source": [
        "# Take the validation directory to make predictions on\n",
        "directory = 'tracksplit/val'\n",
        "\n",
        "# Lists to store the actual and predicted classes\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Encode the labels\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Iterate over every subdirectory in the directory\n",
        "for subdir in os.listdir(directory):\n",
        "    subdir_path = os.path.join(directory, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "        # Iterate over every image in the subdirectory\n",
        "        for filename in os.listdir(subdir_path):\n",
        "            if filename.endswith(\".png\"):\n",
        "                # Load the image\n",
        "                img_path = os.path.join(subdir_path, filename)\n",
        "                img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "                # Convert the image to a numpy array\n",
        "                x = image.img_to_array(img)\n",
        "\n",
        "                # Expand the dimensions to match the shape that the model expects\n",
        "                x = np.expand_dims(x, axis=0)\n",
        "\n",
        "                # Preprocess the input\n",
        "                x = resnet_v2.preprocess_input(x)\n",
        "\n",
        "                # Make a prediction\n",
        "                predictions = model.predict(x)\n",
        "\n",
        "                # Get the index of the class with the highest probability\n",
        "                predicted_class = np.argmax(predictions)\n",
        "\n",
        "                # Add the actual and predicted classes to the lists\n",
        "                # The actual class is the name of the subdirectory\n",
        "                y_true.append(subdir)\n",
        "                y_pred.append(predicted_class)\n",
        "\n",
        "                continue\n",
        "            else:\n",
        "                continue\n",
        "# Transform the labels to numerical labels\n",
        "y_true = le.fit_transform(y_true)\n",
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2inWOjvopC1w",
        "outputId": "f7977734-b461-4412-cbe4-602a3ebd27ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded label 0 corresponds to original label 'd'\n",
            "Encoded label 1 corresponds to original label 'f'\n",
            "Encoded label 2 corresponds to original label 'h'\n",
            "Encoded label 3 corresponds to original label 'j'\n",
            "Encoded label 4 corresponds to original label 'q'\n"
          ]
        }
      ],
      "source": [
        "# Display the encoded labels with the original labels\n",
        "for i, class_name in enumerate(le.classes_):\n",
        "    print(f\"Encoded label {i} corresponds to original label '{class_name}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zDaovliAyY4",
        "outputId": "51a4ff36-f30b-49cc-a927-74ecc966c411"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save('trackv3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "wehgEal_rGFX",
        "outputId": "bcea82f2-5cd0-4c53-ee2c-81134c75408a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this confusion matrix we can see that our model mostly performs well on all classes. There are a few labels wrongly predicted. The maximum amount of wrongly predicted labels for one class is 8 and the minimum is 3. \n",
        "\n",
        "The most mistakes are made in predicting 'right' and 'correct left'. 'Right' was predicted as 'forward' sometimes, and for 'correct left' the model confused it for 'right' the most. Which makes sense and isn't that big of a problem since 'right' and 'correct left' should both go right. \n",
        "\n",
        "The class that performs the best is 'left' and then 'correct right'. The model confuses 'left' the most for 'forward' and 'correct right' for 'correct left', which is a problem but luckily it was only wrong in 4 cases.\n",
        "\n",
        "The class 'forward' predicted 5 times the wrong class. It sees it sometimes as 'left' or 'right'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model that will be used to control Mario\n",
        "model = load_model('trackv3.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's run Mario!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let the model predict based on an image, which direction to go\n",
        "def determine_direction(img):\n",
        "    # Resize the image\n",
        "    img = img.resize((224, 224))\n",
        "    # Convert the image to a numpy array\n",
        "    x = image.img_to_array(img)\n",
        "    # Expand the dimensions to match the shape that the model expects\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # Preprocess the input\n",
        "    x = resnet_v2.preprocess_input(x)\n",
        "    # Let the model predict the direction\n",
        "    x = model.predict(x)\n",
        "    # Set a threshold\n",
        "    threshold = 0.60\n",
        "    # Get the predicted value\n",
        "    action_index = np.argmax(x)\n",
        "    print(action_index)\n",
        "    # Retrieve the confidence score for the predicted value\n",
        "    confidence = x[0][action_index]\n",
        "    print(confidence)\n",
        "    # When the threshold is bigger than the confidence, return 6 so the else statement will be triggered later on\n",
        "    # Else, return the predicted class\n",
        "    if threshold > confidence:\n",
        "        print('back')\n",
        "        return 6\n",
        "    return action_index\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the codec using VideoWriter_fourcc() and create a VideoWriter object\n",
        "# Sleep so we are able to open the game\n",
        "time.sleep(5)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (1920,1080))\n",
        "sct = mss.mss()\n",
        "monitor_info = sct.monitors[0]\n",
        "\n",
        "# Define dimensions of the screen we want to capture\n",
        "top_margin = (monitor_info[\"height\"] // 4) + 50\n",
        "monitor = {\n",
        "    \"top\": monitor_info[\"top\"] + top_margin,\n",
        "    \"left\": monitor_info[\"left\"] + 55,\n",
        "    \"width\": monitor_info[\"width\"] - 120,\n",
        "    \"height\": (monitor_info[\"height\"] * 2 // 3) - 450}\n",
        "\n",
        "# Stop when space is pressed\n",
        "while not keyboard.is_pressed(\"space\"):\n",
        "    # Capture computer screen\n",
        "    screenshot = sct.grab(monitor)\n",
        "    img = np.array(screenshot)\n",
        "    # Create an image from the screenshot and save it in memory\n",
        "    img = Image.frombytes('RGB', screenshot.size, screenshot.rgb)\n",
        "    bytes_io = io.BytesIO()\n",
        "    img.save(bytes_io, format='PNG') \n",
        "    # Predict the direction\n",
        "    direction = determine_direction(img)\n",
        "    #0=d, 1=f, 2=h, 3=j, 4=q\n",
        "    if (direction == 0):\n",
        "        # Go right\n",
        "        print('right')\n",
        "        pyautogui.mouseDown()\n",
        "        keyboard.press('d')\n",
        "        time.sleep(0.35)\n",
        "        keyboard.release('d')\n",
        "        pyautogui.mouseUp()\n",
        "    elif (direction == 4):\n",
        "        # Go left\n",
        "        print('left')\n",
        "        pyautogui.mouseDown()\n",
        "        keyboard.press('q')\n",
        "        time.sleep(0.3)\n",
        "        keyboard.release('q')\n",
        "        pyautogui.mouseUp()\n",
        "    elif (direction == 1):\n",
        "        # Go forward\n",
        "        print('forward')\n",
        "        pyautogui.mouseDown()  \n",
        "        time.sleep(0.5)           \n",
        "        pyautogui.mouseUp()\n",
        "    elif (direction == 2): \n",
        "        # Correct the right - go left\n",
        "        print('c-right')\n",
        "        pyautogui.mouseDown()\n",
        "        keyboard.press('q')\n",
        "        time.sleep(0.3)\n",
        "        keyboard.release('q')\n",
        "        pyautogui.mouseUp()\n",
        "    elif (direction == 3): \n",
        "        # Correct the left - go right\n",
        "        print('c-left')\n",
        "        pyautogui.mouseDown()\n",
        "        keyboard.press('d')\n",
        "        time.sleep(0.3)\n",
        "        keyboard.release('d')\n",
        "        pyautogui.mouseUp()\n",
        "    else: \n",
        "        # Go backwardds\n",
        "        pyautogui.mouseDown(button='right')\n",
        "        time.sleep(0.4)\n",
        "        pyautogui.mouseUp(button='right')\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Release everything when job is finished\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problems we encountered\n",
        "\n",
        "1. Preprocessing the images\n",
        "\n",
        "After multiple attempts where we were trying to get Mario to drive but he wasn’t, we started thinking the problem might be our data. We collected data several times and made sure to clean it well. But even then the issue remained. To check whether the problem was with our dataset or not, we fed our data to Google Teachable Machine and the model we got was surprisingly good. So that confirmed that the issue was not, in fact, from our data. We decided to check the model again as the problem seemed to lie there. But we couldn’t find any issue, so we consulted the teacher about it. Apparently the issue was with how we were preprocessing the images. The way we were doing it when training the model was different from when starting the game and taking screenshots while driving. To check on how the predictions were being made, we made a confusion matrix. \n",
        "After carefully looking at the code once more, specifically at the preprocessing part, we found that we weren't using this when making predictions: resnet_v2.preprocess_input(x). This processes the image into the format that the ResNet model needs. When we fixed that Mario actually started driving, so problem fixed. Here’s a comparison in the confusion matrixes before and after...\n",
        "\n",
        "- Old Confusion Matrix:\n",
        "\n",
        "<img src=\"oldConfusionMatrix.jpg\" width=\"700\"/>\n",
        "\n",
        "- New Confusion Matrix:\n",
        "\n",
        "<img src=\"newConfusionMatrix.jpg\" width=\"700\"/>\n",
        "\n",
        "We learn from this that image preprocessing is a vital step when making the model. And the way you do it during training should match the way it’s done when capturing images during testing.\n",
        "\n",
        "2. Getting the correct data we needed (data suited for the track)\n",
        "\n",
        "Cleaning the data (images) wasn’t the easiest. Not all the images taken matched the label they were assigned to (the folder they were stored in). So we had to make sure that this data was properly cleaned so that it would match the correct move to take on this specific track.\n",
        "\n",
        "3. Dealing with sharp turns\n",
        "\n",
        "Another problem we encountered was that Mario might not know how hard he needs to turn to the left or right. Some turns are wide and large and some are smaller. We thought of adding 2 additional classes which were meant to take a turn when the road was diagonal. This could also help if Mario was in a poor angle after a turn, helping him get back on track instead of going forward outside of the track. After testing this, changing the speed didn't really help but in general implementing these two classes did make the model perform slightly better. The problem with this might be that there wasn't enough data of the large turns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, we can say that Mario can drive some parts of this specific course on his own, but in some parts of the track he still has a hard time figuring out which direction he should go. Our model definitely wasn't the best for driving altough the accuracy and confusion matrix looked really good. We tried to train a Google Teachable Machine model for our data and while the confusion matrix for that model also looked good, it wasn't able to drive perfectly. Maybe this means that Mario Kart is too hard to control for a deep learning model. We even tried it with a different track, but this was even a bit worse then the current track we are using which is the same as the OpenCV part.\n",
        "\n",
        "Throughout this assignment we learned how to use ResNet50v2 to train a model and to use preprocessing on the image. \n",
        "\n",
        "What we would have done differently is probably that we would have chosen a different game since Mario is a bit too hard, since the model performs well but when it actually needs to drive it doesn't perform that great, even with the Teachable Machine model. Maybe we need even more data and when we had a bit more time we could have tried that.\n",
        "\n",
        "We are proud of the way Mario can sometimes follow the track and that he sometimes knows what direction he needs to go to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Alternate Approach\n",
        "\n",
        "A different approach we wanted to attempt was to change the way we were handling and labelling our data. The concept would be to instead of having folders with labels we would have labels in the file names. These file names would contain which button(s) were pressed and the duration of how long they were pressed. This could potentially help Mario scale both large and small turns while keeping him on track. This still leaves the risk of Mario going off track depending on his angle after a turn. Another issue is the duration of how forward(left mouse click) would be handled, as generally in the game it would be pressed for long durations if not through the entire course. However it is unsure how effective this approach would be without first implementing it. Unfortunately we ran out of time before we could implement this approach."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mss",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
